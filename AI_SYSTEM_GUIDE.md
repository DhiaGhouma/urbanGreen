# ğŸ¯ AI Recommendation System - Guide Complet

## âœ… Solution Finale : Serveur HTTP Persistent

**ProblÃ¨me rÃ©solu** : Windows Python 3.12 asyncio bug (OSError WinError 10106)  
**Solution** : Serveur HTTP qui garde le modÃ¨le en mÃ©moire (beaucoup plus rapide)

---

## ğŸš€ DÃ©marrage Rapide

### 1. DÃ©marrer le serveur AI (obligatoire)

```powershell
# Option 1 : Via artisan (recommandÃ©)
php artisan ai:start-server

# Option 2 : Directement avec Python
.\.venv\Scripts\python.exe ai_server.py
```

**Le serveur prend ~10 secondes Ã  dÃ©marrer** (chargement du modÃ¨le sentence-transformers).  
Une fois dÃ©marrÃ©, il affiche :

```
ğŸš€ AI Recommendation Server running on http://127.0.0.1:8765
```

### 2. DÃ©marrer Laravel

```powershell
php artisan serve
```

### 3. Utiliser l'IA

-   Aller sur : http://127.0.0.1:8000/participations/create
-   Cliquer : **"SuggÃ©rer avec l'IA"**
-   L'IA sÃ©lectionne le meilleur greenspace en ~2-3 secondes

---

## ğŸ“Š Performance

| Moteur                | Temps de rÃ©ponse | QualitÃ©                                           |
| --------------------- | ---------------- | ------------------------------------------------- |
| **embeddings_only**   | 2-3s             | â­â­â­â­ TrÃ¨s bon                                 |
| **ollama+embeddings** | 20-25s           | â­â­â­â­â­ Excellent (explications intelligentes) |

---

## ğŸ§ª Tests

### VÃ©rifier que le serveur AI fonctionne

```powershell
# Test santÃ© serveur
Invoke-WebRequest -Uri 'http://127.0.0.1:8765/health'

# Test complet via Laravel
php artisan test:fast-ai
```

**RÃ©sultat attendu** :

```
âœ… Success! (2000-3000ms)
Best Match ID | 3
Score         | 0.56
Engine        | embeddings_only
```

### Test depuis le navigateur

1. Connecte-toi : http://127.0.0.1:8000/login
2. Va sur : http://127.0.0.1:8000/participations/create
3. Clique : "SuggÃ©rer avec l'IA"
4. L'IA retourne le meilleur greenspace avec explication

---

## ğŸ› ï¸ Architecture

### Fichiers ClÃ©s

```
urbanGreen/
â”œâ”€â”€ ai_server.py                      # ğŸ”¥ Serveur AI HTTP (Ã  lancer en premier)
â”œâ”€â”€ app/Services/FastAIRecommender.php # Service Laravel qui appelle le serveur
â”œâ”€â”€ app/Http/Controllers/
â”‚   â””â”€â”€ ParticipationController.php   # Controller avec route suggest()
â””â”€â”€ app/Console/Commands/
    â”œâ”€â”€ StartAIServer.php             # Commande: php artisan ai:start-server
    â””â”€â”€ TestFastAI.php                # Commande: php artisan test:fast-ai
```

### Flow de DonnÃ©es

```
[User clicks "SuggÃ©rer"]
    â†“
[ParticipationController::suggest()]
    â†“
[FastAIRecommender::recommend()]  â† HTTP call
    â†“
[ai_server.py:8765]
    â”œâ”€ Charge sentence-transformers (dÃ©jÃ  en mÃ©moire)
    â”œâ”€ Calcule similaritÃ© sÃ©mantique
    â”œâ”€ Tente Ollama (si disponible)
    â””â”€ Retourne: best_match_id, score, reason
    â†“
[JSON response to browser]
```

---

## ğŸ¨ Technologies UtilisÃ©es

-   **sentence-transformers** : `paraphrase-multilingual-MiniLM-L12-v2`
-   **Embeddings** : SimilaritÃ© cosinus (numpy)
-   **Ollama** : llama3.1 (optionnel, pour explications avancÃ©es)
-   **Serveur** : Python HTTPServer
-   **Laravel** : Http client avec timeout 35s

---

## ğŸ”§ Configuration

### Variables d'environnement (.env)

```env
AI_SERVER_URL=http://127.0.0.1:8765
```

### Port du serveur AI

DÃ©fini dans `ai_server.py` ligne 14 :

```python
PORT = 8765  # Change si conflit
```

---

## âš¡ Optimisations

### Avec Ollama (explications intelligentes)

1. **DÃ©marre Ollama** :

```powershell
Start-Process -FilePath 'C:\Users\moeta\AppData\Local\Programs\Ollama\ollama.exe' -ArgumentList 'serve' -WindowStyle Hidden
```

2. **VÃ©rifie qu'il fonctionne** :

```powershell
Invoke-RestMethod -Uri 'http://127.0.0.1:11434/api/generate' -Method Post -Body '{"model":"llama3.1","prompt":"Test","stream":false}' -ContentType 'application/json'
```

3. **Restart le serveur AI** pour qu'il utilise Ollama

**Temps de rÃ©ponse** : ~20-25s (embeddings 2-3s + Ollama 18-22s)

### Sans Ollama (plus rapide)

Garde seulement les embeddings. L'IA dit :

> "Jardin Botanique correspond le mieux Ã  vos prÃ©fÃ©rences (57% de similaritÃ©)"

**Temps de rÃ©ponse** : ~2-3s

---

## ğŸ› Troubleshooting

### Erreur : "AI server is not available"

**Solution** :

```powershell
php artisan ai:start-server
```

### Erreur : "OSError [WinError 10106]"

**Cause** : Tu utilises l'ancien `AIRecommender` (subprocess)  
**Solution** : Le controller utilise maintenant `FastAIRecommender` (HTTP) âœ…

### Serveur AI lent au dÃ©marrage

**Normal** : Le modÃ¨le sentence-transformers prend ~10s Ã  charger la premiÃ¨re fois.  
Une fois chargÃ©, les requÃªtes sont rapides (2-3s).

### Timeout aprÃ¨s 35 secondes

**Cause** : Ollama prend trop de temps  
**Solution** : Soit augmente timeout dans `FastAIRecommender.php` ligne 56 :

```php
$response = Http::timeout(60)  // Ã©tait 35
```

Soit dÃ©sactive Ollama pour utiliser seulement embeddings (2-3s).

---

## ğŸ“ Commandes Utiles

```powershell
# DÃ©marrer serveur AI
php artisan ai:start-server

# Tester l'IA
php artisan test:fast-ai

# Voir les logs Laravel
Get-Content storage/logs/laravel.log -Tail 50

# VÃ©rifier santÃ© serveur AI
Invoke-WebRequest http://127.0.0.1:8765/health

# ArrÃªter tous les processus Python
Get-Process python | Stop-Process -Force
```

---

## ğŸ‰ RÃ©sumÃ©

âœ… **sentence-transformers** installÃ© et fonctionnel  
âœ… **Embeddings avancÃ©s** (multilingual, 384 dimensions)  
âœ… **Serveur HTTP persistent** (pas de subprocess lent)  
âœ… **Windows asyncio bug** contournÃ©  
âœ… **Temps de rÃ©ponse** : 2-3s (embeddings) ou 20-25s (avec Ollama)  
âœ… **Fallback intelligent** si Ollama indisponible

**Tu es prÃªt ! ğŸš€**
